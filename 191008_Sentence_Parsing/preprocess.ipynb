{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 삼성 DS AI Expert Program\n",
    "\n",
    "## Sentence Parsing Assignment\n",
    "\n",
    "담당 조교 : 문승준 (june1212@kaist.ac.kr)\n",
    "\n",
    "실습 일시: 2019년 10월 8일 (화), 13:30 - 17:30\n",
    "\n",
    "## Introduction\n",
    "\n",
    "본 실습에서는 Tensorflow와 Keras를 이용하여, Sentence Parsing을 직접 진행해 볼 것입니다. Sentence Parsing은 문장 구성 요소 사이의 연관성을 찾아서 기계의 자연어 이해를 돕는 task로, 실습을 통해 직접 구현해보겠습니다. 이번 실습에서 사용되는 자연어 처리 모델은 Seq2Seq와 Transformer입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocess\n",
    "\n",
    "Training을 하기에 앞서, pretrain된 word embedding과 dataset을 load하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 불러오기\n",
    "\n",
    "Train dataset과 Test dataset을 불러옵니다. Demo이기 때문에 실제 dataset보다 훨씬 작은 크기로 구축되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/hankook/Samsung-AI-KAIST/master/191008_Sentence_Parsing/train.tsv\n",
    "!wget https://raw.githubusercontent.com/hankook/Samsung-AI-KAIST/master/191008_Sentence_Parsing/test.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source와 target에 포함되는 vocab dataset을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir vocab\n",
    "!wget -P ./vocab https://raw.githubusercontent.com/hankook/Samsung-AI-KAIST/master/191008_Sentence_Parsing/vocab/source.txt\n",
    "!wget -P ./vocab https://raw.githubusercontent.com/hankook/Samsung-AI-KAIST/master/191008_Sentence_Parsing/vocab/target.txt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-train된 word embedding은 glove.300d를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P ./vocab http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "!unzip ./vocab/glove.840B.300d.zip -d ./vocab/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_counter = Counter()\n",
    "dec_counter = Counter()\n",
    "\n",
    "with open('./train.tsv') as f:\n",
    "  for line in f:\n",
    "    line = line.rstrip()\n",
    "    text_raw, text_tokenized, label = line.split('\\t')\n",
    "    enc_counter.update(text_tokenized.lower().split())\n",
    "    dec_counter.update(label.replace('[', '[ ').lower().split())\n",
    "\n",
    "with open('./vocab/source.txt', 'w') as f:\n",
    "  f.write('<pad>\\n')\n",
    "  for (w, freq) in enc_counter.most_common():\n",
    "    f.write(w+'\\n')\n",
    "    \n",
    "with open('./vocab/target.txt', 'w') as f:\n",
    "  f.write('<pad>\\n')\n",
    "  f.write('<start>\\n')\n",
    "  f.write('<end>\\n')\n",
    "  for (w, freq) in dec_counter.most_common():\n",
    "    f.write(w+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain 된 Embedding 만들기\n",
    "\n",
    "glove.840B.300d.txt 파일을 이용해 word.npy 파일을 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "word2idx = {}\n",
    "with open('./vocab/target.txt', encoding='utf-8') as f:\n",
    "  for i, line in enumerate(f):\n",
    "    line = line.rstrip()\n",
    "    word2idx[line] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = np.zeros((len(word2idx)+1, 300)) # + 1 은 unknown word를 포함하기 위해서 더해준다.\n",
    "\n",
    "with open('./vocab/glove.840B.300d.txt', encoding='utf-8') as f:\n",
    "  count = 0\n",
    "  for i, line in enumerate(f):\n",
    "    if i % 100000 == 0:\n",
    "      print('- At line {}'.format(i))\n",
    "    line = line.rstrip()\n",
    "    sp = line.split(' ')\n",
    "    word, vec = sp[0], sp[1:]\n",
    "    if word in word2idx:\n",
    "      count += 1\n",
    "      embedding[word2idx[word]] = np.asarray(vec, dtype='float32')\n",
    "      \n",
    "print(\"[%d / %d] words have found pre-trained values\"%(count, len(word2idx)))\n",
    "np.save('./vocab/word.npy', embedding)\n",
    "print('Saved ./vocab/word.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
