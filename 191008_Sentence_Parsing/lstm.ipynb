{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 삼성 DS AI Expert Program\n",
    "\n",
    "## Sentence Parsing Assignment\n",
    "\n",
    "담당 조교 : 문승준 (june1212@kaist.ac.kr)\n",
    "\n",
    "실습 일시: 2019년 10월 8일 (화), 13:30 - 17:30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentence Parsing with LSTM seq2seq\n",
    "\n",
    "이번 파트에서는 LSTM seq2seq를 이용해서 Sentence Parsing을 진행해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-gpu==2.0.0-beta0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pprint\n",
    "import logging\n",
    "import time\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset load 하기\n",
    "\n",
    "다음 함수들을 정의해서 Dataset을 load해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(f_path, params):\n",
    "  with open(f_path) as f:\n",
    "    print('Reading', f_path)\n",
    "    for line in f:\n",
    "      text_raw, text_tokenized, label = line.split('\\t')\n",
    "      text_tokenized = text_tokenized.lower().split()\n",
    "      label = label.replace('[', '[ ').lower().split()\n",
    "      source = [params['tgt2idx'].get(w, len(params['tgt2idx'])) for w in text_tokenized]\n",
    "      target = [params['tgt2idx'].get(w, len(params['tgt2idx'])) for w in label]\n",
    "      target_in = [1] + target\n",
    "      target_out = target + [2]\n",
    "      yield (source, target_in, target_out) #iterative하게 접근할 수 있도록 yield 함수를 사용해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(is_training, params):\n",
    "  _shapes = ([None], [None], [None])\n",
    "  _types = (tf.int32, tf.int32, tf.int32)\n",
    "  _pads = (0, 0, 0)\n",
    "  \n",
    "  if is_training:\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "      lambda: data_generator(params['train_path'], params),\n",
    "      output_shapes = _shapes,\n",
    "      output_types = _types,)\n",
    "    ds = ds.shuffle(params['buffer_size'])\n",
    "    ds = ds.padded_batch(params['train_batch_size'], _shapes, _pads)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "  else:\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "      lambda: data_generator(params['test_path'], params),\n",
    "      output_shapes = _shapes,\n",
    "      output_types = _types,)\n",
    "    ds = ds.padded_batch(params['eval_batch_size'], _shapes, _pads)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "  \n",
    "  return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder 정의하기\n",
    "\n",
    "Seq2seq 모델의 Encoder 부분을 먼저 정의해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, params):\n",
    "    super().__init__()\n",
    "    \n",
    "    ###########\n",
    "    \n",
    "    self.embedding = \n",
    "    \n",
    "    # preprocess 과정에서 정의한 word.npy를 이용해 embedding될 수 있도록 해줍니다.\n",
    "    \n",
    "    ###########\n",
    "    \n",
    "    self.dropout = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "    self.encoder = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(params['rnn_units']//2,\n",
    "                                                                      return_state=True,\n",
    "                                                                      return_sequences=True))\n",
    "  \n",
    "  def call(self, inputs, training=False):\n",
    "    if inputs.dtype != tf.int32:\n",
    "      inputs = tf.cast(inputs, tf.int32)\n",
    "    \n",
    "    x = tf.nn.embedding_lookup(self.embedding, inputs)\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    encoder_outputs, state_fw_h, state_fw_c, state_bw_h, state_bw_c = self.encoder(x)\n",
    "    \n",
    "    state = (tf.concat((state_fw_h, state_bw_h), -1),\n",
    "             tf.concat((state_fw_c, state_bw_c), -1),)\n",
    "    state = tf.concat(state, -1)\n",
    "    \n",
    "    return (encoder_outputs, [state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention 정의하기\n",
    "\n",
    "Decoder 부분에서 사용할 attention을 정의해줍니다. 여기서는 처음 attention decoder를 적용한 논문에서 소개된 BahdanauAttention을 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "    self.b = self.add_weight(shape=[units], name='bias')\n",
    "    self.V = tf.keras.layers.Dense(1, use_bias=False)\n",
    "    \n",
    "  def call(self, query, values, values_mask):\n",
    "    query = tf.expand_dims(query, 1)\n",
    "\n",
    "    score = self.V(tf.tanh(self.W1(values) + self.W2(query) + self.b))\n",
    "    score = tf.squeeze(score, -1)\n",
    "\n",
    "    # pre-softmax masking\n",
    "    paddings = tf.fill(tf.shape(score), float('-inf'))\n",
    "    score = tf.where(tf.equal(values_mask, 0), paddings, score)\n",
    "    \n",
    "    align = tf.nn.softmax(score, axis=1)\n",
    "    align = tf.expand_dims(align, -1)\n",
    "    \n",
    "    context_vector = tf.matmul(values, align, transpose_a=True)\n",
    "    context_vector = tf.squeeze(context_vector, -1)\n",
    "\n",
    "    return context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effective Approaches to Attention-based Neural Machine Translation에 소개된 LuongAttention은 BahdanauAttention보다 조금 더 좋은 성능을 나타낸다고 합니다.\n",
    "\n",
    "Link : https://arxiv.org/pdf/1508.04025.pdf\n",
    "\n",
    "## Assignment 1 : LuongAttention 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    #### Assignment 1-1 ####\n",
    "\n",
    "  def call(self, query, values, values_mask):\n",
    "    \n",
    "    #### Assignment 1-2 ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder 정의하기\n",
    "\n",
    "Decoder 부분을 정의해줍니다. 여기 decoder layer에서는 attention을 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, params, tied_embedding):\n",
    "    super().__init__()\n",
    "    self.embedding = tied_embedding\n",
    "    self.attention = BahdanauAttention(params['rnn_units'])\n",
    "    self.dropout = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "    self.cell = tf.keras.layers.StackedRNNCells([\n",
    "      tf.keras.layers.LSTMCell(params['rnn_units']),\n",
    "      tf.keras.layers.LSTMCell(params['rnn_units']),\n",
    "    ])\n",
    "    self.out_bias = self.add_weight(name='out_bias', shape=[len(params['tgt2idx'])+1])\n",
    "  \n",
    "  \n",
    "  @tf.function\n",
    "  def call(self, inputs, training=False):\n",
    "    inputs, states, memory, memory_mask = inputs\n",
    "    \n",
    "    if inputs.dtype != tf.int32:\n",
    "      inputs = tf.cast(inputs, tf.int32)\n",
    "    \n",
    "    h0, c0, h1, c1 = tf.split(states, 4, axis=-1)\n",
    "    \n",
    "    context_vector = self.attention(h1, memory, memory_mask)\n",
    "    \n",
    "    x = tf.nn.embedding_lookup(self.embedding, inputs)\n",
    "    \n",
    "    x = tf.concat([context_vector, x], axis=-1)\n",
    "    \n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    output, states = self.cell(x, ((h0, c0), (h1, c1)))\n",
    "    \n",
    "    logits = tf.matmul(output, self.embedding, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, self.out_bias)\n",
    "    \n",
    "    states = tf.concat([states[0][0], states[0][1], states[1][0], states[1][1]], axis=-1)\n",
    "    \n",
    "    return logits, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "아래 함수처럼 한 step의 train을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(source, target_in, target_out, encoder, decoder, params):\n",
    "  loss = 0\n",
    "  logits = []\n",
    "  encoder_outputs, decoder_state = encoder(source, training=True)\n",
    "  decoder_state = tf.concat(decoder_state + decoder_state, -1)\n",
    "  \n",
    "  for t in range(target_in.shape[1]):\n",
    "    _logits, decoder_state = decoder([target_in[:, t],\n",
    "                                      decoder_state,\n",
    "                                      encoder_outputs,\n",
    "                                      tf.sign(source)],\n",
    "                                      training=True)\n",
    "    logits.append(_logits)\n",
    "  \n",
    "  logits = tf.stack(logits, 1)\n",
    "  \n",
    "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=target_out, logits=logits)\n",
    "  weights = tf.cast(tf.sign(target_in), tf.float32)\n",
    "  loss = tf.reduce_sum(loss * weights) / tf.reduce_sum(weights)\n",
    "  \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미리 pre-train된 vocabulary set(dictionary)을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(f_path):\n",
    "  word2idx = {}\n",
    "  with open(f_path, encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "      line = line.rstrip()\n",
    "      word2idx[line] = i\n",
    "  return word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early-stop을 위해서는 accuarcy가 감소하는지 체크해야합니다. 이를 위해서 아래와 같은 함수를 정의해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_descending(history: list) -> bool:\n",
    "  history = history[-(params['num_patience']+1):]\n",
    "  for i in range(1, len(history)):\n",
    "    if history[i-1] <= history[i]:\n",
    "      return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter setting\n",
    "\n",
    "Train에 필요한 hyperparameter들을 아래처럼 dictionary로 정의해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'train_path': './train.tsv',\n",
    "    'test_path': './test.tsv',\n",
    "    'vocab_src_path': './vocab/source.txt',\n",
    "    'vocab_tgt_path': './vocab/target.txt',\n",
    "    'model_path': './model/',\n",
    "    'dropout_rate': 0.2,\n",
    "    'rnn_units': 300,\n",
    "    'max_decode_len': 50,\n",
    "    'lr': 4e-4,\n",
    "    'clip_norm': .1,\n",
    "    'buffer_size': 31279,\n",
    "    'train_batch_size': 32,\n",
    "    'eval_batch_size': 128,\n",
    "    'num_patience': 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['tgt2idx'] = get_vocab(params['vocab_tgt_path'])\n",
    "params['idx2tgt'] = {idx: tgt for tgt, idx in params['tgt2idx'].items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 만들기\n",
    "\n",
    "위에서 설정한 hyperparameter를 이용해서 model을 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bidirectional_3/forward_lstm_3/kernel:0', TensorShape([300, 600])),\n",
      " ('bidirectional_3/forward_lstm_3/recurrent_kernel:0', TensorShape([150, 600])),\n",
      " ('bidirectional_3/forward_lstm_3/bias:0', TensorShape([600])),\n",
      " ('bidirectional_3/backward_lstm_3/kernel:0', TensorShape([300, 600])),\n",
      " ('bidirectional_3/backward_lstm_3/recurrent_kernel:0',\n",
      "  TensorShape([150, 600])),\n",
      " ('bidirectional_3/backward_lstm_3/bias:0', TensorShape([600])),\n",
      " ('pretrained_embedding:0', TensorShape([8692, 300]))]\n",
      "[('bahdanau_attention_3/dense_9/kernel:0', TensorShape([300, 300])),\n",
      " ('bahdanau_attention_3/dense_10/kernel:0', TensorShape([300, 300])),\n",
      " ('bahdanau_attention_3/dense_11/kernel:0', TensorShape([300, 1])),\n",
      " ('bias:0', TensorShape([300])),\n",
      " ('stacked_rnn_cells_3/kernel:0', TensorShape([600, 1200])),\n",
      " ('stacked_rnn_cells_3/recurrent_kernel:0', TensorShape([300, 1200])),\n",
      " ('stacked_rnn_cells_3/bias:0', TensorShape([1200])),\n",
      " ('stacked_rnn_cells_3/kernel:0', TensorShape([300, 1200])),\n",
      " ('stacked_rnn_cells_3/recurrent_kernel:0', TensorShape([300, 1200])),\n",
      " ('stacked_rnn_cells_3/bias:0', TensorShape([1200])),\n",
      " ('pretrained_embedding:0', TensorShape([8692, 300])),\n",
      " ('out_bias:0', TensorShape([8692]))]\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(params)\n",
    "encoder.build((None, None))\n",
    "pprint.pprint([(v.name, v.shape) for v in encoder.trainable_variables])\n",
    "\n",
    "decoder = Decoder(params, encoder.embedding)\n",
    "decoder.build([[None], [None, 4*params['rnn_units']], [None, None, params['rnn_units']], [None, None]])\n",
    "pprint.pprint([(v.name, v.shape) for v in decoder.trainable_variables])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learing Rate와 optimizer를 정의해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_lr = tf.optimizers.schedules.ExponentialDecay(params['lr'], 1000, 0.96)\n",
    "optim = tf.optimizers.Adam(params['lr'])\n",
    "global_step = 0\n",
    "history_acc = []\n",
    "best_acc = .0\n",
    "t0 = time.time()\n",
    "logger = logging.getLogger('tensorflow')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임의의 문장에 대해서 현재까지 학습된 모델이 어떻게 parsing을 진행하는지 확인할 수 있도록, 아래와 같은 함수를 정의해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimal_test(encoder, decoder, params):\n",
    "  test_str = ['what', 'times', 'are', 'the', 'nutcracker', 'show', 'playing', 'near', 'me']\n",
    "  test_arr = tf.convert_to_tensor([[params['tgt2idx'][w] for w in test_str]])\n",
    "  generated = tf.convert_to_tensor([[1]])\n",
    "  ids = generated[0]\n",
    "  \n",
    "  encoder_outputs, decoder_state = encoder(test_arr, training=False)\n",
    "  decoder_state = tf.concat(decoder_state + decoder_state, -1)\n",
    "  \n",
    "  for i in range(params['max_decode_len']):\n",
    "    logits, decoder_state = decoder([ids,\n",
    "                                     decoder_state,\n",
    "                                     encoder_outputs,\n",
    "                                     tf.sign(test_arr)],\n",
    "                                     training=False)\n",
    "    ids = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "    generated = tf.concat((generated, tf.expand_dims(ids, 1)), axis=1)\n",
    "    if np.asscalar(ids.numpy()) == 2:\n",
    "      break\n",
    "  \n",
    "  print('-'*12)\n",
    "  print('minimal test')\n",
    "  print('utterance:', ' '.join(test_str))\n",
    "  parsed = ' '.join([params['idx2tgt'][idx] for idx in generated[0].numpy()[1:-1]])\n",
    "  print('parsed:', parsed)\n",
    "  print()\n",
    "  try:\n",
    "    nltk.tree.Tree.fromstring(parsed.replace('[ ', '(').replace(' ]', ')')).pretty_print()\n",
    "  except:\n",
    "    pass\n",
    "  print('-'*12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training을 시작해봅시다\n",
    "\n",
    "실제 training을 시작해봅시다. Train set과 test set이 실제보다 크게 작아서 원하는 정확도는 얻기 힘들 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "while True:\n",
    "  # TRAINING\n",
    "  for (source, target_in, target_out) in dataset(is_training=True, params=params):\n",
    "    with tf.GradientTape() as tape:\n",
    "      loss = train_step(source, target_in, target_out, encoder, decoder, params)\n",
    "      \n",
    "    optim.lr.assign(decay_lr(global_step))\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    grads, _ = tf.clip_by_global_norm(grads, params['clip_norm'])\n",
    "    optim.apply_gradients(zip(grads, variables))\n",
    "    \n",
    "    if global_step % 10 == 0:\n",
    "      logger.info(\"Step {} | Loss: {:.4f} | Spent: {:.1f} secs | LR: {:.6f}\".format(\n",
    "          global_step, loss.numpy().item(), time.time()-t0, optim.lr.numpy().item()))\n",
    "      t0 = time.time()\n",
    "    \n",
    "    global_step += 1\n",
    "  if global_step % 10 == 0:\n",
    "    # EVALUATION\n",
    "    is_training=False\n",
    "    minimal_test(encoder, decoder, params)\n",
    "    m = tf.keras.metrics.Mean()\n",
    "    \n",
    "    for i, (source, target_in, target_out) in enumerate(dataset(is_training=is_training, params=params)):\n",
    "      generated = tf.ones((source.shape[0], 1), tf.int32)\n",
    "      ids = tf.squeeze(generated, axis=1)\n",
    "      encoder_outputs, decoder_state = encoder(source, training=is_training)\n",
    "      decoder_state = tf.concat(decoder_state + decoder_state, -1)\n",
    "      \n",
    "      for j in range(target_out.shape[1]):\n",
    "        logits, decoder_state = decoder([ids,\n",
    "                                         decoder_state,\n",
    "                                         encoder_outputs,\n",
    "                                         tf.sign(source)],\n",
    "                                         training=is_training)\n",
    "        ids = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "        generated = tf.concat((generated, tf.expand_dims(ids, 1)), axis=1)\n",
    "\n",
    "      seq_lens = tf.argmax(tf.cast(tf.equal(target_out, 2), tf.int32), axis=1)\n",
    "      for pred, tgt, seq_len in zip(generated.numpy(), target_out.numpy(), seq_lens.numpy()):\n",
    "        pred = pred[1:][:seq_len+1]\n",
    "        tgt = tgt[:seq_len+1]\n",
    "        matched = np.all(pred == tgt)\n",
    "        m.update_state(int(matched))\n",
    "    \n",
    "    acc = m.result().numpy()\n",
    "    logger.info(\"Evaluation: Testing Exact Match Accuracy: {:.3f}\".format(acc))\n",
    "    history_acc.append(acc)\n",
    "\n",
    "    if acc > best_acc:\n",
    "      best_acc = acc\n",
    "    logger.info(\"Best Accuracy: {:.3f}\".format(best_acc))\n",
    "\n",
    "    if len(history_acc) > params['num_patience'] and is_descending(history_acc):\n",
    "      logger.info(\"Testing Accuracy not improved over {} epochs, Early Stop\".format(params['num_patience']))\n",
    "      break\n",
    "  else:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Seq2seq with GRU\n",
    "\n",
    "앞 실습에서는 seq2seq의 Base RNN unit을 LSTM으로 사용했습니다. 이를 GRU로 대체해서 Encoder, Decoder를 새롭게 설계해서 제출해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    \n",
    "    #### Assignment 2-1 ####\n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    \n",
    "    #### Assignment 2-2 ####\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
