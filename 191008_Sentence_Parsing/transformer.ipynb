{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 삼성 DS AI Expert Program\n",
    "\n",
    "## Sentence Parsing Assignment\n",
    "\n",
    "담당 조교 : 문승준 (june1212@kaist.ac.kr)\n",
    "\n",
    "실습 일시: 2019년 10월 8일 (화), 13:30 - 17:30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentence Parsing with Transformer\n",
    "\n",
    "이번 파트에서는 Transformer를 이용해서 Sentence Parsing을 진행해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pprint\n",
    "import logging\n",
    "import time\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-gpu==2.0.0-alpha0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset load 하기\n",
    "\n",
    "다음 함수들을 정의해서 Dataset을 load해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(f_path, params):\n",
    "  with open(f_path) as f:\n",
    "    print('Reading', f_path)\n",
    "    for line in f:\n",
    "      text_raw, text_tokenized, label = line.split('\\t')\n",
    "      text_tokenized = text_tokenized.lower().split()\n",
    "      label = label.replace('[', '[ ').lower().split()\n",
    "      source = [params['tgt2idx'].get(w, len(params['tgt2idx'])) for w in text_tokenized]\n",
    "      target = [params['tgt2idx'].get(w, len(params['tgt2idx'])) for w in label]\n",
    "      target_in = [1] + target\n",
    "      target_out = target + [2]\n",
    "      yield (source, target_in, target_out) #iterative하게 접근할 수 있도록 yield 함수를 사용해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(is_training, params):\n",
    "  _shapes = ([None], [None], [None])\n",
    "  _types = (tf.int32, tf.int32, tf.int32)\n",
    "  _pads = (0, 0, 0)\n",
    "  \n",
    "  if is_training:\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "      lambda: data_generator(params['train_path'], params),\n",
    "      output_shapes = _shapes,\n",
    "      output_types = _types,)\n",
    "    ds = ds.shuffle(params['train_samples'])\n",
    "    ds = ds.padded_batch(params['train_batch_size'], _shapes, _pads)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "  else:\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "      lambda: data_generator(params['test_path'], params),\n",
    "      output_shapes = _shapes,\n",
    "      output_types = _types,)\n",
    "    ds = ds.padded_batch(params['eval_batch_size'], _shapes, _pads)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "  \n",
    "  return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "다음 함수를 통해서 positional encoding을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timing_signal_1d(length,\n",
    "                         channels,\n",
    "                         min_timescale=1.0,\n",
    "                         max_timescale=1.0e4,\n",
    "                         start_index=0):\n",
    "  to_float = lambda x: tf.cast(x, tf.float32)\n",
    "  position = to_float(tf.range(length) + start_index)\n",
    "  num_timescales = channels // 2\n",
    "  log_timescale_increment = (\n",
    "      tf.math.log(float(max_timescale) / float(min_timescale)) /\n",
    "      tf.maximum(to_float(num_timescales) - 1, 1))\n",
    "  inv_timescales = min_timescale * tf.exp(\n",
    "      to_float(tf.range(num_timescales)) * -log_timescale_increment)\n",
    "  scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(inv_timescales, 0)\n",
    "  signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "  signal = tf.pad(signal, [[0, 0], [0, tf.math.floormod(channels, 2)]])\n",
    "  signal = tf.reshape(signal, [1, length, channels])\n",
    "  return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization\n",
    "\n",
    "다음 코드를 통해서 Layer Normalization을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(tf.keras.layers.Layer):\n",
    "  def __init__(self, params):\n",
    "    super().__init__()\n",
    "    self._epsilon = params['epsilon']\n",
    "    self._hidden_units = params['global_units']\n",
    "  \n",
    "  def build(self, input_shape):\n",
    "    self.scale = self.add_weight(name='scale',\n",
    "                                 shape=[self._hidden_units],\n",
    "                                 initializer=tf.ones_initializer(),\n",
    "                                 trainable=True)\n",
    "    self.bias = self.add_weight(name='bias',\n",
    "                                shape=[self._hidden_units],\n",
    "                                initializer=tf.zeros_initializer(),\n",
    "                                trainable=True)\n",
    "    super().build(input_shape)\n",
    "  \n",
    "  def call(self, inputs):\n",
    "    mean, variance = tf.nn.moments(inputs, [-1], keepdims=True)\n",
    "    norm_x = (inputs - mean) * tf.math.rsqrt(variance + self._epsilon)\n",
    "    return norm_x * self.scale + self.bias\n",
    "  \n",
    "  def compute_output_shape(self, input_shape):\n",
    "    return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointwiseFFNBlock(tf.keras.Model):\n",
    "  def __init__(self, params):\n",
    "    super().__init__()\n",
    "    self.layer_norm = LayerNorm(params)\n",
    "    self.block_dropout = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "\n",
    "    self.filter = tf.keras.layers.Dense(params['multiplier']*params['global_units'], tf.nn.relu, name='filter')\n",
    "    self.dropout = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "    self.linear = tf.keras.layers.Dense(params['global_units'], name='linear')\n",
    "  \n",
    "  \n",
    "  def call(self, inputs, training=False):\n",
    "    x = self.layer_norm(inputs)\n",
    "    x = self.forward(x, training=training)\n",
    "    x = self.block_dropout(x, training=training)\n",
    "    x += inputs\n",
    "    return x\n",
    "    \n",
    "  \n",
    "  def forward(self, x, training):\n",
    "    return self.linear(self.dropout(self.filter(x), training=training))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention 정의하기\n",
    "\n",
    "Transformer의 가장 핵심이 되는 부분, self attention 부분을 정의해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionBlock(tf.keras.Model):\n",
    "  def __init__(self, params, is_bidirectional):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.layer_norm = LayerNorm(params)\n",
    "    self.block_dropout = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "    \n",
    "    self.qkv_linear = tf.keras.layers.Dense(3 * params['global_units'], name='qkv_linear')\n",
    "    self.dropout = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "    self.out_linear = tf.keras.layers.Dense(params['global_units'], name='out_linear')\n",
    "    \n",
    "    self._is_bidirectional = is_bidirectional\n",
    "    self._num_heads = params['num_heads']\n",
    "  \n",
    "  \n",
    "  def call(self, inputs, training=False):\n",
    "    inputs, masks = inputs\n",
    "    x = self.layer_norm(inputs)\n",
    "    x = self.forward((x, masks), training=training)\n",
    "    x = self.block_dropout(x, training=training)\n",
    "    x += inputs\n",
    "    return x\n",
    "    \n",
    "  \n",
    "  def forward(self, inputs, training):\n",
    "    x, masks = inputs\n",
    "    timesteps = tf.shape(x)[1]\n",
    "    \n",
    "    q_k_v = self.qkv_linear(x)\n",
    "    q, k, v = tf.split(q_k_v, 3, axis=-1)\n",
    "    \n",
    "    if self._num_heads > 1:\n",
    "      q = tf.concat(tf.split(q, self._num_heads, axis=2), axis=0)                        \n",
    "      k = tf.concat(tf.split(k, self._num_heads, axis=2), axis=0)                        \n",
    "      v = tf.concat(tf.split(v, self._num_heads, axis=2), axis=0)\n",
    "    \n",
    "    #### fill your code for self-attention ####\n",
    "    \n",
    "    align = \n",
    "    \n",
    "    ####\n",
    "    \n",
    "    if (masks is not None) or (not self._is_bidirectional):\n",
    "      paddings = tf.fill(tf.shape(align), float('-inf'))\n",
    "    \n",
    "    if masks is not None:\n",
    "      c_masks = tf.tile(masks, [params['num_heads'], 1])\n",
    "      c_masks = tf.tile(tf.expand_dims(c_masks, 1), [1, timesteps, 1])\n",
    "      align = tf.where(tf.equal(c_masks, 0), paddings, align)\n",
    "    \n",
    "    if not self._is_bidirectional:\n",
    "      lower_tri = tf.ones((timesteps, timesteps))                                       \n",
    "      lower_tri = tf.linalg.LinearOperatorLowerTriangular(lower_tri).to_dense()      \n",
    "      t_masks = tf.tile(tf.expand_dims(lower_tri, 0), [tf.shape(align)[0], 1, 1])     \n",
    "      align = tf.where(tf.equal(t_masks, 0), paddings, align)\n",
    "    \n",
    "    align = tf.nn.softmax(align)\n",
    "    align = self.dropout(align, training=training)\n",
    "    \n",
    "    if masks is not None:\n",
    "      q_masks = tf.tile(masks, [params['num_heads'], 1])\n",
    "      q_masks = tf.tile(tf.expand_dims(q_masks, 2), [1, 1, timesteps])\n",
    "      align *= tf.cast(q_masks, tf.float32)\n",
    "    \n",
    "    x = tf.matmul(align, v)\n",
    "    if self._num_heads > 1:\n",
    "      x = tf.concat(tf.split(x, self._num_heads, axis=0), axis=2)\n",
    "    x = self.out_linear(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder layer에서 필요한 Mutual Attention Block도 정의해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutualAttentionBlock(tf.keras.Model):\n",
    "  def __init__(self, params):\n",
    "    super().__init__()\n",
    "    self.layer_norm = LayerNorm(params)\n",
    "    self.block_dropout = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "    \n",
    "    self.q_linear = tf.keras.layers.Dense(params['global_units'], name='q_linear')\n",
    "    self.kv_linear = tf.keras.layers.Dense(2*params['global_units'], name='kv_linear')\n",
    "    self.dropout = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "    self.out_linear = tf.keras.layers.Dense(params['global_units'], name='out_linear')\n",
    "    \n",
    "    self._num_heads = params['num_heads']\n",
    "  \n",
    "  \n",
    "  def call(self, inputs, training=False):\n",
    "    inputs, mask_dec, encoded, mask_enc = inputs\n",
    "    x = self.layer_norm(inputs)\n",
    "    x = self.forward((x, mask_dec, encoded, mask_enc), training=training)\n",
    "    x = self.block_dropout(x, training=training)\n",
    "    x += inputs\n",
    "    return x\n",
    "    \n",
    "  \n",
    "  def forward(self, inputs, training):\n",
    "    query, mask_query, context, mask_context = inputs\n",
    "    time_query, time_context = tf.shape(query)[1], tf.shape(context)[1]\n",
    "    \n",
    "    q = self.q_linear(query)\n",
    "    k_v = self.kv_linear(context)\n",
    "    k, v = tf.split(k_v, 2, axis=-1)\n",
    "    \n",
    "    if self._num_heads > 1:\n",
    "      q = tf.concat(tf.split(q, self._num_heads, axis=2), axis=0)                        \n",
    "      k = tf.concat(tf.split(k, self._num_heads, axis=2), axis=0)                        \n",
    "      v = tf.concat(tf.split(v, self._num_heads, axis=2), axis=0)\n",
    "    \n",
    "    #### fill your code for self-attention ####\n",
    "    \n",
    "    align = \n",
    "    \n",
    "    ####\n",
    "    \n",
    "    paddings = tf.fill(tf.shape(align), float('-inf'))\n",
    "    context_masks = tf.tile(mask_context, [self._num_heads, 1])\n",
    "    context_masks = tf.tile(tf.expand_dims(context_masks, 1), [1, time_query, 1])\n",
    "    align = tf.where(tf.equal(context_masks, 0), paddings, align)\n",
    "    \n",
    "    align = tf.nn.softmax(align)\n",
    "    align = self.dropout(align, training=training)\n",
    "    \n",
    "    query_masks = tf.tile(mask_query, [self._num_heads, 1])\n",
    "    query_masks = tf.tile(tf.expand_dims(query_masks, 2), [1, 1, time_context])\n",
    "    align *= tf.cast(query_masks, tf.float32)\n",
    "    \n",
    "    x = tf.matmul(align, v)\n",
    "    if self._num_heads > 1:\n",
    "      x = tf.concat(tf.split(x, self._num_heads, axis=0), axis=2)\n",
    "    x = self.out_linear(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder, Decoder Layer 정의하기\n",
    "\n",
    "모델의 Encoder layer, Decoder layer를 먼저 정의해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.Model):\n",
    "  def __init__(self, params, name):\n",
    "    super().__init__(name=name)\n",
    "    self.self_attention = SelfAttentionBlock(params, is_bidirectional=True)\n",
    "    self.pointwise_ffn = PointwiseFFNBlock(params)\n",
    "  \n",
    "  \n",
    "  def call(self, inputs, training=False):\n",
    "    x, mask = inputs\n",
    "    x = self.self_attention((x, mask), training=training)\n",
    "    x = self.pointwise_ffn(x, training=training)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.Model):\n",
    "  def __init__(self, params, name):\n",
    "    super().__init__(name=name)\n",
    "    self.self_attention = SelfAttentionBlock(params, is_bidirectional=False)\n",
    "    self.mutual_attention = MutualAttentionBlock(params)\n",
    "    self.pointwise_ffn = PointwiseFFNBlock(params)\n",
    "  \n",
    "  \n",
    "  def call(self, inputs, training=False):\n",
    "    decoded, mask_dec, encoded, mask_enc = inputs\n",
    "    decoded = self.self_attention((decoded, mask_dec), training=training)\n",
    "    decoded = self.mutual_attention((decoded, mask_dec, encoded, mask_enc), training=training)\n",
    "    decoded = self.pointwise_ffn(decoded, training=training)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder 정의하기\n",
    "\n",
    "Encoder는 Positional Encoding + EncoderLayer의 stack 구조로 이루어져 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, params):\n",
    "    super().__init__()\n",
    "    self.params = params\n",
    "    self.embedding = tf.keras.layers.Embedding(len(params['tgt2idx'])+1, params['global_units'],\n",
    "      embeddings_initializer=tf.initializers.RandomNormal(stddev=params['global_units'] ** -0.5))\n",
    "    self.input_dropout = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "    self.encodes = [EncoderLayer(params, name='enc_layer_{}'.format(i+1)) for i in range(params['num_layers'])]\n",
    "    \n",
    "  \n",
    "  def call(self, inputs, training=False):\n",
    "    input_enc = inputs\n",
    "    \n",
    "    mask_enc = tf.sign(input_enc)\n",
    "    \n",
    "    input_enc = self.embedding(input_enc)\n",
    "    \n",
    "    encoded = self.input_transform(input_enc, training=training)\n",
    "    \n",
    "    for layer in self.encodes:\n",
    "      encoded = layer((encoded, mask_enc), training=training)\n",
    "    \n",
    "    return encoded\n",
    "  \n",
    "  \n",
    "  def input_transform(self, x, training):\n",
    "    if self.params['is_embedding_scaled']:\n",
    "      x *= tf.sqrt(tf.cast(self.params['global_units'], tf.float32))\n",
    "    x += get_timing_signal_1d(tf.shape(x)[1], self.params['global_units'])\n",
    "    x = self.input_dropout(x, training=training)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder 정의하기\n",
    "\n",
    "Encoder는 DecoderLayer의 stack 구조로 이루어져 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, params, tied_embedding):\n",
    "    super().__init__()\n",
    "    self.params = params\n",
    "    self.embedding = tied_embedding\n",
    "    self.input_dropout = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "    self.decodes = [DecoderLayer(params, name='dec_layer_{}'.format(i+1)) for i in range(params['num_layers'])]\n",
    "    self.out_bias = self.add_weight(name='out_bias', shape=[len(params['tgt2idx'])+1])\n",
    "    \n",
    "  \n",
    "  def call(self, inputs, training=False):\n",
    "    input_dec, memory, memory_mask = inputs\n",
    "    \n",
    "    mask_dec = tf.sign(input_dec)\n",
    "    \n",
    "    input_dec = self.embedding(input_dec)\n",
    "    \n",
    "    decoded = self.input_transform(input_dec, training=training)\n",
    "    \n",
    "    for layer in self.decodes:\n",
    "      decoded = layer((decoded, mask_dec, memory, memory_mask), training=training)\n",
    "    \n",
    "    logits = self.tied_output(decoded)\n",
    "    return logits\n",
    "  \n",
    "  \n",
    "  def input_transform(self, x, training):\n",
    "    if self.params['is_embedding_scaled']:\n",
    "      x *= tf.sqrt(tf.cast(self.params['global_units'], tf.float32))\n",
    "    x += get_timing_signal_1d(tf.shape(x)[1], self.params['global_units'])\n",
    "    x = self.input_dropout(x, training=training)\n",
    "    return x\n",
    "  \n",
    "  \n",
    "  def tied_output(self, decoded):\n",
    "    axis_1, axis_2  = tf.shape(decoded)[0], tf.shape(decoded)[1]\n",
    "    decoded = tf.reshape(decoded, (axis_1*axis_2, params['global_units']))\n",
    "    logits = tf.matmul(decoded, self.embedding.embeddings, transpose_b=True)\n",
    "    logits = tf.reshape(logits, (axis_1, axis_2, len(self.params['tgt2idx'])+1))\n",
    "    logits = tf.nn.bias_add(logits, self.out_bias)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미리 pre-train된 vocabulary set(dictionary)을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(f_path):\n",
    "  word2idx = {}\n",
    "  with open(f_path, encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "      line = line.rstrip()\n",
    "      word2idx[line] = i\n",
    "  return word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early-stop을 위해서는 accuarcy가 감소하는지 체크해야합니다. 이를 위해서 아래와 같은 함수를 정의해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_descending(history: list) -> bool:\n",
    "  history = history[-(params['num_patience']+1):]\n",
    "  for i in range(1, len(history)):\n",
    "    if history[i-1] <= history[i]:\n",
    "      return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter setting\n",
    "\n",
    "Train에 필요한 hyperparameter들을 아래처럼 dictionary로 정의해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'train_path': './train.tsv',\n",
    "    'test_path': './test.tsv',\n",
    "    'vocab_src_path': './vocab/source.txt',\n",
    "    'vocab_tgt_path': './vocab/target.txt',\n",
    "    'model_path': './model/',\n",
    "    'num_layers': 3,\n",
    "    'dropout_rate': 0.2,\n",
    "    'global_units': 300,\n",
    "    'num_heads': 4,\n",
    "    'multiplier': 2,\n",
    "    'epsilon': 1e-6,\n",
    "    'is_embedding_scaled': True,\n",
    "    'max_decode_len': 50,\n",
    "    'lr': 4e-4,\n",
    "    'train_samples': 31279,\n",
    "    'train_batch_size': 32,\n",
    "    'eval_samples': 9042,\n",
    "    'eval_batch_size': 300,\n",
    "    'num_patience': 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['tgt2idx'] = get_vocab(params['vocab_tgt_path'])\n",
    "params['idx2tgt'] = {idx: tgt for tgt, idx in params['tgt2idx'].items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 만들기\n",
    "\n",
    "위에서 설정한 hyperparameter를 이용해서 model을 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(params['model_path']).mkdir(exist_ok=True)\n",
    "\n",
    "encoder = Encoder(params)\n",
    "encoder.build((None, None))\n",
    "pprint.pprint([(v.name, v.shape) for v in encoder.trainable_variables])\n",
    "\n",
    "decoder = Decoder(params, encoder.embedding)\n",
    "decoder.build([[None, None], [None, None, params['global_units']], [None, None]])\n",
    "pprint.pprint([(v.name, v.shape) for v in decoder.trainable_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_lr = tf.optimizers.schedules.ExponentialDecay(params['lr'], 1000, 0.96)\n",
    "optim = tf.optimizers.Adam(params['lr'])\n",
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_acc = []\n",
    "best_acc = .0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "logger = logging.getLogger('tensorflow')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임의의 문장에 대해서 현재까지 학습된 모델이 어떻게 parsing을 진행하는지 확인할 수 있도록, 아래와 같은 함수를 정의해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimal_test(encoder, decoder, params):\n",
    "  test_str = ['what', 'times', 'are', 'the', 'nutcracker', 'show', 'playing', 'near', 'me']\n",
    "  test_arr = tf.convert_to_tensor([[params['tgt2idx'][w] for w in test_str]])\n",
    "  generated = tf.convert_to_tensor([[1]])\n",
    "  memory = encoder(test_arr, training=False)\n",
    "  memory_mask = tf.sign(test_arr)\n",
    "  \n",
    "  for i in range(params['max_decode_len']):\n",
    "    logits = decoder((generated, memory, memory_mask), training=False)\n",
    "    ids = tf.argmax(logits[:, i, :], axis=-1, output_type=tf.int32)\n",
    "    ids = tf.expand_dims(ids, 1)\n",
    "    generated = tf.concat((generated, ids), axis=1)\n",
    "    if np.asscalar(ids.numpy()) == 2:\n",
    "      break\n",
    "  print('-'*12)\n",
    "  print('minimal test')\n",
    "  print('utterance:', ' '.join(test_str))\n",
    "  parsed = ' '.join([params['idx2tgt'][idx] for idx in generated[0].numpy()[1:-1]])\n",
    "  print('parsed:', parsed)\n",
    "  print()\n",
    "  try:\n",
    "    nltk.tree.Tree.fromstring(parsed.replace('[ ', '(').replace(' ]', ')')).pretty_print()\n",
    "  except:\n",
    "    pass\n",
    "  print('-'*12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training을 시작해봅시다\n",
    "\n",
    "실제 training을 시작해봅시다. Train set과 test set이 실제보다 크게 작아서 원하는 정확도는 얻기 힘들 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "  # TRAINING\n",
    "  is_training = True\n",
    "  for (source, target_in, target_out) in dataset(is_training=is_training, params=params):\n",
    "    with tf.GradientTape() as tape:\n",
    "      memory = encoder(source, training=is_training)\n",
    "      logits = decoder((target_in, memory, tf.sign(source)), training=is_training)\n",
    "      loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target_out, logits=logits)\n",
    "      weights = tf.cast(tf.sign(target_out), tf.float32)\n",
    "      loss = tf.reduce_sum(loss * weights) / tf.reduce_sum(weights)\n",
    "      \n",
    "    optim.lr.assign(decay_lr(global_step))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    optim.apply_gradients(zip(grads, variables))\n",
    "    \n",
    "    if global_step % 10 == 0:\n",
    "      logger.info(\"Step {} | Loss: {:.4f} | Spent: {:.1f} secs | LR: {:.6f}\".format(\n",
    "          global_step, loss.numpy().item(), time.time()-t0, optim.lr.numpy().item()))\n",
    "      t0 = time.time()\n",
    "    \n",
    "    global_step += 1\n",
    "  if global_step % 1 == 0:\n",
    "    # EVALUATION\n",
    "    minimal_test(encoder, decoder, params)\n",
    "    m = tf.keras.metrics.Mean()\n",
    "    is_training=False\n",
    "    \n",
    "    for i, (source, target_in, target_out) in enumerate(dataset(is_training=is_training, params=params)):\n",
    "      generated = tf.ones((source.shape[0], 1), tf.int32)\n",
    "      memory = encoder(source, training=is_training)\n",
    "      memory_mask = tf.sign(source)\n",
    "      \n",
    "      for j in range(target_out.shape[1]):\n",
    "        logits = decoder((generated, memory, memory_mask), training=is_training)\n",
    "        ids = tf.argmax(logits[:, j, :], axis=-1, output_type=tf.int32)\n",
    "        ids = tf.expand_dims(ids, 1)\n",
    "        generated = tf.concat((generated, ids), axis=1)\n",
    "\n",
    "      seq_lens = tf.argmax(tf.cast(tf.equal(target_out, 2), tf.int32), axis=1)\n",
    "      for pred, tgt, seq_len in zip(generated.numpy(), target_out.numpy(), seq_lens.numpy()):\n",
    "        pred = pred[1:][:seq_len+1]\n",
    "        tgt = tgt[:seq_len+1]\n",
    "        matched = np.all(pred == tgt)\n",
    "        m.update_state(int(matched))\n",
    "      print(\"Testing [{}/{}]\".format(i, params['eval_samples']//params['eval_batch_size']))\n",
    "    \n",
    "    acc = m.result().numpy()\n",
    "    logger.info(\"Evaluation: Testing Exact Match Accuracy: {:.3f}\".format(acc))\n",
    "    history_acc.append(acc)\n",
    "\n",
    "    if acc > best_acc:\n",
    "      best_acc = acc\n",
    "      encoder.save_weights(os.path.join(params['model_path'], 'encoder_{}'.format(global_step)))\n",
    "      decoder.save_weights(os.path.join(params['model_path'], 'decoder_{}'.format(global_step)))\n",
    "    logger.info(\"Best Accuracy: {:.3f}\".format(best_acc))\n",
    "\n",
    "    if len(history_acc) > params['num_patience'] and is_descending(history_acc):\n",
    "      logger.info(\"Testing Accuracy not improved over {} epochs, Early Stop\".format(params['num_patience']))\n",
    "      break\n",
    "  else:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Positional Encoding of Transformer\n",
    "\n",
    "Transformer의 Positional Encoding은 삼각함수를 통해 만들어졌습니다. 하지만 이 외에도 Positional Encoding을 할 수 있는 방법은 여러가지가 있을 것입니다. 본인만의 Positional Encoding 방식을 생각해서 코드와 함께 제출해주세요."
    "\n"
    "(Assignment 3는 과제 점수에는 포함되지 않지만, 제출해주신 분에 대해서는 가산점을 드릴 예정입니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positionalencoding(length, channels, start_index):\n",
    "    \n",
    "    #### your code here ####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
