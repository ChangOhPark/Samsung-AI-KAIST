{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 삼성 DS AI Expert Program\n",
    "\n",
    "## Sentence Parsing Assignment\n",
    "\n",
    "담당 조교 : 문승준 (june1212@kaist.ac.kr)\n",
    "\n",
    "실습 일시: 2019년 10월 8일 (화), 13:30 - 17:30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentence Parsing with LSTM seq2seq\n",
    "\n",
    "이번 파트에서는 LSTM seq2seq를 이용해서 Sentence Parsing을 진행해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-gpu==1.14.0\n",
    "# !pip install tensorflow-gpu==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/com24/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/com24/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/com24/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/com24/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/com24/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/com24/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/com24/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/com24/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/com24/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/com24/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/com24/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/com24/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pprint\n",
    "import logging\n",
    "import time\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset load 하기\n",
    "\n",
    "다음 함수들을 정의해서 Dataset을 load해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(f_path, params):\n",
    "  with open(f_path) as f:\n",
    "    print('Reading', f_path)\n",
    "    for line in f:\n",
    "      text_raw, text_tokenized, label = line.split('\\t')\n",
    "      text_tokenized = text_tokenized.lower().split()\n",
    "      label = label.replace('[', '[ ').lower().split()\n",
    "      source = [params['tgt2idx'].get(w, len(params['tgt2idx'])) for w in text_tokenized]\n",
    "      target = [params['tgt2idx'].get(w, len(params['tgt2idx'])) for w in label]\n",
    "      target_in = [1] + target\n",
    "      target_out = target + [2]\n",
    "      yield (source, target_in, target_out) #iterative하게 접근할 수 있도록 yield 함수를 사용해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(is_training, params):\n",
    "  _shapes = ([None], [None], [None])\n",
    "  _types = (tf.int32, tf.int32, tf.int32)\n",
    "  _pads = (0, 0, 0)\n",
    "  \n",
    "  if is_training:\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "      lambda: data_generator(params['train_path'], params),\n",
    "      output_shapes = _shapes,\n",
    "      output_types = _types,)\n",
    "    ds = ds.shuffle(params['buffer_size'])\n",
    "    ds = ds.padded_batch(params['train_batch_size'], _shapes, _pads)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "  else:\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "      lambda: data_generator(params['test_path'], params),\n",
    "      output_shapes = _shapes,\n",
    "      output_types = _types,)\n",
    "    ds = ds.padded_batch(params['eval_batch_size'], _shapes, _pads)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "  \n",
    "  return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder 정의하기\n",
    "\n",
    "Seq2seq 모델의 Encoder 부분을 먼저 정의해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imbedding_npy = np.load('vocab/word.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imbedding_npy.shape)\n",
    "print(imbedding_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, params):\n",
    "    super().__init__()\n",
    "    \n",
    "    ###########\n",
    "    imbedding_npy = np.load('vocab/word.npy')\n",
    "    \n",
    "    self.embedding = tf.Variable(name=\"embedding_space\",\n",
    "                                 initial_value=imbedding_npy,\n",
    "                                 dtype=tf.float32,\n",
    "                                 trainable=False)\n",
    "    \n",
    "    # preprocess 과정에서 정의한 word.npy를 이용해 embedding될 수 있도록 해줍니다.\n",
    "    \n",
    "    ###########\n",
    "    \n",
    "    self.dropout = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "    self.encoder = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(params['rnn_units']//2,\n",
    "                                                                      return_state=True,\n",
    "                                                                      return_sequences=True))\n",
    "  \n",
    "  def call(self, inputs, training=False):\n",
    "    if inputs.dtype != tf.int32:\n",
    "      inputs = tf.cast(inputs, tf.int32)\n",
    "    print('')\n",
    "    \n",
    "    x = tf.nn.embedding_lookup(self.embedding, inputs)\n",
    "    \n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    encoder_outputs, state_fw_h, state_fw_c, state_bw_h, state_bw_c = self.encoder(x)\n",
    "    \n",
    "    state = (tf.concat((state_fw_h, state_bw_h), -1),\n",
    "             tf.concat((state_fw_c, state_bw_c), -1),)\n",
    "    state = tf.concat(state, -1)\n",
    "    \n",
    "    return (encoder_outputs, [state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention 정의하기\n",
    "\n",
    "Decoder 부분에서 사용할 attention을 정의해줍니다. 여기서는 처음 attention decoder를 적용한 논문에서 소개된 BahdanauAttention을 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "    self.b = self.add_weight(shape=[units], name='bias')\n",
    "    self.V = tf.keras.layers.Dense(1, use_bias=False)\n",
    "    \n",
    "  def call(self, query, values, values_mask):\n",
    "    query = tf.expand_dims(query, 1)\n",
    "\n",
    "    score = self.V(tf.tanh(self.W1(values) + self.W2(query) + self.b))\n",
    "    score = tf.squeeze(score, -1)\n",
    "\n",
    "    # pre-softmax masking\n",
    "    paddings = tf.fill(tf.shape(score), float('-inf'))\n",
    "    score = tf.where(tf.equal(values_mask, 0), paddings, score)\n",
    "    \n",
    "    align = tf.nn.softmax(score, axis=1)\n",
    "    align = tf.expand_dims(align, -1)\n",
    "    \n",
    "    context_vector = tf.matmul(values, align, transpose_a=True)\n",
    "    context_vector = tf.squeeze(context_vector, -1)\n",
    "\n",
    "    return context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effective Approaches to Attention-based Neural Machine Translation에 소개된 LuongAttention은 BahdanauAttention보다 조금 더 좋은 성능을 나타낸다고 합니다.\n",
    "\n",
    "Link : https://arxiv.org/pdf/1508.04025.pdf\n",
    "\n",
    "## Assignment 1 : LuongAttention 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    #### Assignment 1-1 ####\n",
    "    super(LuongAttention, self).__init__()\n",
    "    \n",
    "\n",
    "  def call(self, query, values, values_mask):\n",
    "    \n",
    "    #### Assignment 1-2 ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder 정의하기\n",
    "\n",
    "Decoder 부분을 정의해줍니다. 여기 decoder layer에서는 attention을 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, params, tied_embedding):\n",
    "    super().__init__()\n",
    "    self.embedding = tied_embedding\n",
    "    self.attention = BahdanauAttention(params['rnn_units'])\n",
    "    self.dropout = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "    self.cell = tf.keras.layers.StackedRNNCells([\n",
    "      tf.keras.layers.LSTMCell(params['rnn_units']),\n",
    "      tf.keras.layers.LSTMCell(params['rnn_units']),\n",
    "    ])\n",
    "    self.out_bias = self.add_weight(name='out_bias', shape=[len(params['tgt2idx'])+1])\n",
    "  \n",
    "  \n",
    "  @tf.function\n",
    "  def call(self, inputs, training=False):\n",
    "    inputs, states, memory, memory_mask = inputs\n",
    "    \n",
    "    if inputs.dtype != tf.int32:\n",
    "      inputs = tf.cast(inputs, tf.int32)\n",
    "    \n",
    "    h0, c0, h1, c1 = tf.split(states, 4, axis=-1)\n",
    "    \n",
    "    context_vector = self.attention(h1, memory, memory_mask)\n",
    "    \n",
    "    x = tf.nn.embedding_lookup(self.embedding, inputs)\n",
    "    \n",
    "    x = tf.concat([context_vector, x], axis=-1)\n",
    "    \n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    output, states = self.cell(x, ((h0, c0), (h1, c1)))\n",
    "    \n",
    "    logits = tf.matmul(output, self.embedding, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, self.out_bias)\n",
    "    \n",
    "    states = tf.concat([states[0][0], states[0][1], states[1][0], states[1][1]], axis=-1)\n",
    "    \n",
    "    return logits, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "아래 함수처럼 한 step의 train을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(source, target_in, target_out, encoder, decoder, params):\n",
    "  loss = 0\n",
    "  logits = []\n",
    "  encoder_outputs, decoder_state = encoder(source, training=True)\n",
    "  decoder_state = tf.concat(decoder_state + decoder_state, -1)\n",
    "  \n",
    "  for t in range(target_in.shape[1]):\n",
    "    _logits, decoder_state = decoder([target_in[:, t],\n",
    "                                      decoder_state,\n",
    "                                      encoder_outputs,\n",
    "                                      tf.sign(source)],\n",
    "                                      training=True)\n",
    "    logits.append(_logits)\n",
    "  \n",
    "  logits = tf.stack(logits, 1)\n",
    "  \n",
    "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=target_out, logits=logits)\n",
    "  weights = tf.cast(tf.sign(target_in), tf.float32)\n",
    "  loss = tf.reduce_sum(loss * weights) / tf.reduce_sum(weights)\n",
    "  \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미리 pre-train된 vocabulary set(dictionary)을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(f_path):\n",
    "  word2idx = {}\n",
    "  with open(f_path, encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "      line = line.rstrip()\n",
    "      word2idx[line] = i\n",
    "  return word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early-stop을 위해서는 accuarcy가 감소하는지 체크해야합니다. 이를 위해서 아래와 같은 함수를 정의해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_descending(history: list) -> bool:\n",
    "  history = history[-(params['num_patience']+1):]\n",
    "  for i in range(1, len(history)):\n",
    "    if history[i-1] <= history[i]:\n",
    "      return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter setting\n",
    "\n",
    "Train에 필요한 hyperparameter들을 아래처럼 dictionary로 정의해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'train_path': './train.tsv',\n",
    "    'test_path': './test.tsv',\n",
    "    'vocab_src_path': './vocab/source.txt',\n",
    "    'vocab_tgt_path': './vocab/target.txt',\n",
    "    'model_path': './model/',\n",
    "    'dropout_rate': 0.2,\n",
    "    'rnn_units': 300,\n",
    "    'max_decode_len': 50,\n",
    "    'lr': 4e-4,\n",
    "    'clip_norm': .1,\n",
    "    'buffer_size': 31279,\n",
    "    'train_batch_size': 32,\n",
    "    'eval_batch_size': 128,\n",
    "    'num_patience': 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['tgt2idx'] = get_vocab(params['vocab_tgt_path'])\n",
    "params['idx2tgt'] = {idx: tgt for tgt, idx in params['tgt2idx'].items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 만들기\n",
    "\n",
    "위에서 설정한 hyperparameter를 이용해서 model을 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[('bidirectional/forward_lstm/kernel:0',\n",
      "  TensorShape([Dimension(300), Dimension(600)])),\n",
      " ('bidirectional/forward_lstm/recurrent_kernel:0',\n",
      "  TensorShape([Dimension(150), Dimension(600)])),\n",
      " ('bidirectional/forward_lstm/bias:0', TensorShape([Dimension(600)])),\n",
      " ('bidirectional/backward_lstm/kernel:0',\n",
      "  TensorShape([Dimension(300), Dimension(600)])),\n",
      " ('bidirectional/backward_lstm/recurrent_kernel:0',\n",
      "  TensorShape([Dimension(150), Dimension(600)])),\n",
      " ('bidirectional/backward_lstm/bias:0', TensorShape([Dimension(600)]))]\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2ea80559e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2ea80559e8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2ea80559e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2ea80559e8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f2e8c71f160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f2e8c71f160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f2e8c71f160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f2e8c71f160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From <ipython-input-5-a23ee7e24cec>:17: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f2ea800d6a8> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f2ea800d6a8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f2ea800d6a8> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f2ea800d6a8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2ea80559e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2ea80559e8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2ea80559e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2ea80559e8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f2e8c71f160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f2e8c71f160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f2e8c71f160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f2e8c71f160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "[('bahdanau_attention/dense/kernel:0',\n",
      "  TensorShape([Dimension(300), Dimension(300)])),\n",
      " ('bahdanau_attention/dense_1/kernel:0',\n",
      "  TensorShape([Dimension(300), Dimension(300)])),\n",
      " ('bahdanau_attention/dense_2/kernel:0',\n",
      "  TensorShape([Dimension(300), Dimension(1)])),\n",
      " ('bias:0', TensorShape([Dimension(300)])),\n",
      " ('stacked_rnn_cells/kernel:0', TensorShape([Dimension(600), Dimension(1200)])),\n",
      " ('stacked_rnn_cells/recurrent_kernel:0',\n",
      "  TensorShape([Dimension(300), Dimension(1200)])),\n",
      " ('stacked_rnn_cells/bias:0', TensorShape([Dimension(1200)])),\n",
      " ('stacked_rnn_cells/kernel:0', TensorShape([Dimension(300), Dimension(1200)])),\n",
      " ('stacked_rnn_cells/recurrent_kernel:0',\n",
      "  TensorShape([Dimension(300), Dimension(1200)])),\n",
      " ('stacked_rnn_cells/bias:0', TensorShape([Dimension(1200)])),\n",
      " ('out_bias:0', TensorShape([Dimension(8692)]))]\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(params)\n",
    "encoder.build((None, None))\n",
    "pprint.pprint([(v.name, v.shape) for v in encoder.trainable_variables])\n",
    "\n",
    "decoder = Decoder(params, encoder.embedding)\n",
    "decoder.build([[None], [None, 4*params['rnn_units']], [None, None, params['rnn_units']], [None, None]])\n",
    "pprint.pprint([(v.name, v.shape) for v in decoder.trainable_variables])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learing Rate와 optimizer를 정의해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_lr = tf.keras.optimizers.schedules.ExponentialDecay(params['lr'], 1000, 0.96)\n",
    "optim = tf.keras.optimizers.Adam(params['lr'])\n",
    "global_step = 0\n",
    "history_acc = []\n",
    "best_acc = .0\n",
    "t0 = time.time()\n",
    "logger = logging.getLogger('tensorflow')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임의의 문장에 대해서 현재까지 학습된 모델이 어떻게 parsing을 진행하는지 확인할 수 있도록, 아래와 같은 함수를 정의해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimal_test(encoder, decoder, params):\n",
    "  test_str = ['what', 'times', 'are', 'the', 'nutcracker', 'show', 'playing', 'near', 'me']\n",
    "  test_arr = tf.convert_to_tensor([[params['tgt2idx'][w] for w in test_str]])\n",
    "  generated = tf.convert_to_tensor([[1]])\n",
    "  ids = generated[0]\n",
    "  \n",
    "  encoder_outputs, decoder_state = encoder(test_arr, training=False)\n",
    "  decoder_state = tf.concat(decoder_state + decoder_state, -1)\n",
    "  \n",
    "  for i in range(params['max_decode_len']):\n",
    "    logits, decoder_state = decoder([ids,\n",
    "                                     decoder_state,\n",
    "                                     encoder_outputs,\n",
    "                                     tf.sign(test_arr)],\n",
    "                                     training=False)\n",
    "    ids = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "    generated = tf.concat((generated, tf.expand_dims(ids, 1)), axis=1)\n",
    "    if np.asscalar(ids.numpy()) == 2:\n",
    "      break\n",
    "  \n",
    "  print('-'*12)\n",
    "  print('minimal test')\n",
    "  print('utterance:', ' '.join(test_str))\n",
    "  parsed = ' '.join([params['idx2tgt'][idx] for idx in generated[0].numpy()[1:-1]])\n",
    "  print('parsed:', parsed)\n",
    "  print()\n",
    "  try:\n",
    "    nltk.tree.Tree.fromstring(parsed.replace('[ ', '(').replace(' ]', ')')).pretty_print()\n",
    "  except:\n",
    "    pass\n",
    "  print('-'*12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training을 시작해봅시다\n",
    "\n",
    "실제 training을 시작해봅시다. Train set과 test set이 실제보다 크게 작아서 원하는 정확도는 얻기 힘들 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/com24/.local/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:494: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "Reading ./train.tsv\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2ea80559e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2ea80559e8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2ea80559e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2ea80559e8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f2e8c71f160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f2e8c71f160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f2e8c71f160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f2e8c71f160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "INFO:tensorflow:Step 0 | Loss: 8.9669 | Spent: 10.7 secs | LR: 0.000400\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2ea80559e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2ea80559e8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2ea80559e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2ea80559e8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f2e8c71f160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f2e8c71f160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f2e8c71f160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f2e8c71f160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "------------\n",
      "minimal test\n",
      "utterance: what times are the nutcracker show playing near me\n",
      "parsed: [ [ [ [ [ ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ]\n",
      "\n",
      "------------\n",
      "Reading ./test.tsv\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2ea80559e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2ea80559e8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Decoder.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2ea80559e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f2ea80559e8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f2e8c71f160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f2e8c71f160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f2e8c71f160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f2e8c71f160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "INFO:tensorflow:Evaluation: Testing Exact Match Accuracy: 0.000\n",
      "INFO:tensorflow:Best Accuracy: 0.000\n",
      "Reading ./train.tsv\n",
      "\n",
      "INFO:tensorflow:Step 10 | Loss: 6.4188 | Spent: 5.4 secs | LR: 0.000400\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "\n",
      "------------\n",
      "minimal test\n",
      "utterance: what times are the nutcracker show playing near me\n",
      "parsed: [ [ [ [ [ [ ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ]\n",
      "\n",
      "------------\n",
      "Reading ./test.tsv\n",
      "\n",
      "INFO:tensorflow:Evaluation: Testing Exact Match Accuracy: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Best Accuracy: 0.000\n",
      "Reading ./train.tsv\n",
      "\n",
      "INFO:tensorflow:Step 20 | Loss: 5.5908 | Spent: 4.9 secs | LR: 0.000400\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "\n",
      "------------\n",
      "minimal test\n",
      "utterance: what times are the nutcracker show playing near me\n",
      "parsed: [ [ [ what what [ [ [ [ ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ]\n",
      "\n",
      "------------\n",
      "Reading ./test.tsv\n",
      "\n",
      "INFO:tensorflow:Evaluation: Testing Exact Match Accuracy: 0.000\n",
      "INFO:tensorflow:Best Accuracy: 0.000\n",
      "Reading ./train.tsv\n",
      "\n",
      "INFO:tensorflow:Step 30 | Loss: 5.0502 | Spent: 4.9 secs | LR: 0.000400\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "\n",
      "------------\n",
      "minimal test\n",
      "utterance: what times are the nutcracker show playing near me\n",
      "parsed: [ [ what what what [ [ [ [ [ [ ] ] ] ] ] ] ] ] ] the ] ] ] ] tonight ] ] ] tonight ] ] ] tonight ] ] tonight ] ] ] tonight ] ] tonight ] ] tonight ] ]\n",
      "\n",
      "------------\n",
      "Reading ./test.tsv\n",
      "\n",
      "INFO:tensorflow:Evaluation: Testing Exact Match Accuracy: 0.000\n",
      "INFO:tensorflow:Best Accuracy: 0.000\n",
      "Reading ./train.tsv\n",
      "\n",
      "INFO:tensorflow:Step 40 | Loss: 4.4608 | Spent: 5.1 secs | LR: 0.000399\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "\n",
      "------------\n",
      "minimal test\n",
      "utterance: what times are the nutcracker show playing near me\n",
      "parsed: [ [ what what what what [ [ [ [ the [ [ the [ ] ] 's ] ] 's ] 's ] ] 's week ] ] ] released ] firm ] ] firm ] ] firm ] ] tomorrow ] ] ] the tomorrow ] ]\n",
      "\n",
      "------------\n",
      "Reading ./test.tsv\n",
      "\n",
      "INFO:tensorflow:Evaluation: Testing Exact Match Accuracy: 0.000\n",
      "INFO:tensorflow:Best Accuracy: 0.000\n",
      "Reading ./train.tsv\n",
      "\n",
      "INFO:tensorflow:Step 50 | Loss: 3.9051 | Spent: 5.0 secs | LR: 0.000399\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "\n",
      "------------\n",
      "minimal test\n",
      "utterance: what times are the nutcracker show playing near me\n",
      "parsed: [ [ what what what is at [ [ [ hoime [ [ the home ] ] ] ] the the the ] ] released released ] released released ] released released ] released ] released the on ] ] ] released the tomorrow ] ] ] couponing ]\n",
      "\n",
      "------------\n",
      "Reading ./test.tsv\n",
      "\n",
      "INFO:tensorflow:Evaluation: Testing Exact Match Accuracy: 0.000\n",
      "INFO:tensorflow:Best Accuracy: 0.000\n",
      "Reading ./train.tsv\n",
      "\n",
      "INFO:tensorflow:Step 60 | Loss: 3.4577 | Spent: 5.1 secs | LR: 0.000399\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "\n",
      "------------\n",
      "minimal test\n",
      "utterance: what times are the nutcracker show playing near me\n",
      "parsed: [ [ therer what what is at [ [ playing [ [ grandmom the [ ] ] 's 's his ] ] ] 's 's right ] ] megacon ] released ] megacon it ] ] megacon here here ] ] here here here ] here ] released here\n",
      "\n",
      "------------\n",
      "Reading ./test.tsv\n",
      "\n",
      "INFO:tensorflow:Evaluation: Testing Exact Match Accuracy: 0.000\n",
      "INFO:tensorflow:Best Accuracy: 0.000\n",
      "Reading ./train.tsv\n",
      "\n",
      "INFO:tensorflow:Step 70 | Loss: 3.0661 | Spent: 4.5 secs | LR: 0.000399\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "\n",
      "------------\n",
      "minimal test\n",
      "utterance: what times are the nutcracker show playing near me\n",
      "parsed: [ [ therer what what at [ [ arethe [ the the [ home ] ] ] ] 's week week ] ] ] megacon week ] ] megacon now ] megacon ] calms now ] now ] calms now ] calms ] calms ] ] calms now on\n",
      "\n",
      "------------\n",
      "Reading ./test.tsv\n",
      "\n",
      "INFO:tensorflow:Evaluation: Testing Exact Match Accuracy: 0.000\n",
      "INFO:tensorflow:Best Accuracy: 0.000\n",
      "Reading ./train.tsv\n",
      "\n",
      "INFO:tensorflow:Step 80 | Loss: 2.7781 | Spent: 5.2 secs | LR: 0.000399\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "\n",
      "------------\n",
      "minimal test\n",
      "utterance: what times are the nutcracker show playing near me\n",
      "parsed: [ > what what is at [ playing [ constrction [ [ arethe home ] ] ] the the week ] ] ] i29 week ] ] calms it it ] calms ] released on ] starring on ] starring ] stjames weekend ] ] stjames ] bestselling weekend\n",
      "\n",
      "------------\n",
      "Reading ./test.tsv\n",
      "\n",
      "INFO:tensorflow:Evaluation: Testing Exact Match Accuracy: 0.000\n",
      "INFO:tensorflow:Best Accuracy: 0.000\n",
      "Reading ./train.tsv\n",
      "\n",
      "INFO:tensorflow:Step 90 | Loss: 2.5150 | Spent: 4.9 secs | LR: 0.000399\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "\n",
      "------------\n",
      "minimal test\n",
      "utterance: what times are the nutcracker show playing near me\n",
      "parsed: [ esports what what is at [ at [ arethe [ > the the ] ] ] the his ] ] ilka week ] ] released it week ] spray ] starring ] starring it yesterday ] ] starring on ] starring on ] starring weekend ] ] stjames\n",
      "\n",
      "------------\n",
      "Reading ./test.tsv\n",
      "\n",
      "INFO:tensorflow:Evaluation: Testing Exact Match Accuracy: 0.000\n",
      "INFO:tensorflow:Best Accuracy: 0.000\n",
      "Reading ./train.tsv\n",
      "\n",
      "INFO:tensorflow:Step 100 | Loss: 2.3246 | Spent: 5.2 secs | LR: 0.000398\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "\n",
      "------------\n",
      "minimal test\n",
      "utterance: what times are the nutcracker show playing near me\n",
      "parsed: [ > what what is at [ at [ > [ arethe the home ] ] ] ] the what what morning ] ] [ starring released today ] ] released released it on ] ] stjames ] stjames ] released grilling yesterday ] ] stjames it ] released\n",
      "\n",
      "------------\n",
      "Reading ./test.tsv\n",
      "\n",
      "INFO:tensorflow:Evaluation: Testing Exact Match Accuracy: 0.000\n",
      "INFO:tensorflow:Best Accuracy: 0.000\n",
      "Reading ./train.tsv\n",
      "\n",
      "INFO:tensorflow:Step 110 | Loss: 2.1948 | Spent: 5.3 secs | LR: 0.000398\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "\n",
      "------------\n",
      "minimal test\n",
      "utterance: what times are the nutcracker show playing near me\n",
      "parsed: [ > what what is at [ at [ can't [ > the the house ] ] ] ] by what what morning ] [ starring released ] starring released ] released on on ] ] stjames grilling ] ] bestselling it it weekend ] ] abeer yesterday ]\n",
      "\n",
      "------------\n",
      "Reading ./test.tsv\n",
      "\n",
      "INFO:tensorflow:Evaluation: Testing Exact Match Accuracy: 0.000\n",
      "INFO:tensorflow:Best Accuracy: 0.000\n",
      "Reading ./train.tsv\n",
      "\n",
      "INFO:tensorflow:Step 120 | Loss: 2.0652 | Spent: 4.9 secs | LR: 0.000398\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "\n",
      "------------\n",
      "minimal test\n",
      "utterance: what times are the nutcracker show playing near me\n",
      "parsed: [ esports what what [ is at [ arethe [ arethe [ the house ] ] ] the the ] album album ] released [ released released release ] released grill ] ] bestselling it it ] starring ] abeer yesterday ] ] abeer it ] it it ]\n",
      "\n",
      "------------\n",
      "Reading ./test.tsv\n",
      "\n",
      "INFO:tensorflow:Evaluation: Testing Exact Match Accuracy: 0.000\n",
      "INFO:tensorflow:Best Accuracy: 0.000\n",
      "Reading ./train.tsv\n",
      "\n",
      "INFO:tensorflow:Step 130 | Loss: 1.9446 | Spent: 5.2 secs | LR: 0.000398\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "\n",
      "------------\n",
      "minimal test\n",
      "utterance: what times are the nutcracker show playing near me\n",
      "parsed: [ therer what what [ you're at [ arethe [ arethe [ the house ] ] ] thee the album album ] [ released album ] released release ] released grill ] ] stjames grilling ] released it yesterday ] ] starring grilling ] abeer it ] ] abeer\n",
      "\n",
      "------------\n",
      "Reading ./test.tsv\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation: Testing Exact Match Accuracy: 0.000\n",
      "INFO:tensorflow:Best Accuracy: 0.000\n",
      "Reading ./train.tsv\n",
      "\n",
      "INFO:tensorflow:Step 140 | Loss: 1.8372 | Spent: 4.4 secs | LR: 0.000398\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "\n",
      "------------\n",
      "minimal test\n",
      "utterance: what times are the nutcracker show playing near me\n",
      "parsed: [ therer what what [ is at [ arethe [ arethe [ the house ] ] ] ] the what week ] [ starring released ] starring yesterday ] ] starring it it ] starring ] abeer yesterday ] ] abeer it ] ] abeer it yesterday ] ]\n",
      "\n",
      "------------\n",
      "Reading ./test.tsv\n",
      "\n",
      "INFO:tensorflow:Evaluation: Testing Exact Match Accuracy: 0.000\n",
      "INFO:tensorflow:Best Accuracy: 0.000\n",
      "Reading ./train.tsv\n",
      "\n",
      "INFO:tensorflow:Step 150 | Loss: 1.7495 | Spent: 4.6 secs | LR: 0.000398\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "\n",
      "------------\n",
      "minimal test\n",
      "utterance: what times are the nutcracker show playing near me\n",
      "parsed: [ > what what is at [ wonderlands [ 33184 [ the the house ] ] ] ] by what morning ] thinks now ] starring now ] starring ] 90046 ] can't it ] it it ] abeer 55th ] ] morming ] bestseller it it ] bestseller\n",
      "\n",
      "------------\n",
      "Reading ./test.tsv\n",
      "\n",
      "INFO:tensorflow:Evaluation: Testing Exact Match Accuracy: 0.000\n",
      "INFO:tensorflow:Best Accuracy: 0.000\n",
      "Reading ./train.tsv\n",
      "\n",
      "INFO:tensorflow:Step 160 | Loss: 1.6908 | Spent: 4.7 secs | LR: 0.000397\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "\n",
      "------------\n",
      "minimal test\n",
      "utterance: what times are the nutcracker show playing near me\n",
      "parsed: [ therer what what [ is at at [ [ arethe [ the home ] ] ] the the ] album album ] [ stjames the ] ] stjames it ] ] abeer it it ] ] abeer it ] 90046 ] starring it ] starring it ] abeer\n",
      "\n",
      "------------\n",
      "Reading ./test.tsv\n",
      "\n",
      "INFO:tensorflow:Evaluation: Testing Exact Match Accuracy: 0.000\n",
      "INFO:tensorflow:Best Accuracy: 0.000\n",
      "Reading ./train.tsv\n",
      "\n",
      "INFO:tensorflow:Step 170 | Loss: 1.6455 | Spent: 4.7 secs | LR: 0.000397\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n",
      "Reading ./train.tsv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "while True:\n",
    "  # TRAINING\n",
    "  for (source, target_in, target_out) in dataset(is_training=True, params=params):\n",
    "    with tf.GradientTape() as tape:\n",
    "      loss = train_step(source, target_in, target_out, encoder, decoder, params)\n",
    "      \n",
    "    optim.lr.assign(decay_lr(global_step))\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    grads, _ = tf.clip_by_global_norm(grads, params['clip_norm'])\n",
    "    optim.apply_gradients(zip(grads, variables))\n",
    "    \n",
    "    if global_step % 10 == 0:\n",
    "      logger.info(\"Step {} | Loss: {:.4f} | Spent: {:.1f} secs | LR: {:.6f}\".format(\n",
    "          global_step, loss.numpy().item(), time.time()-t0, optim.lr.numpy().item()))\n",
    "      t0 = time.time()\n",
    "    \n",
    "    global_step += 1\n",
    "  if global_step % 10 == 0:\n",
    "    # EVALUATION\n",
    "    is_training=False\n",
    "    minimal_test(encoder, decoder, params)\n",
    "    m = tf.keras.metrics.Mean()\n",
    "    \n",
    "    for i, (source, target_in, target_out) in enumerate(dataset(is_training=is_training, params=params)):\n",
    "      generated = tf.ones((source.shape[0], 1), tf.int32)\n",
    "      ids = tf.squeeze(generated, axis=1)\n",
    "      encoder_outputs, decoder_state = encoder(source, training=is_training)\n",
    "      decoder_state = tf.concat(decoder_state + decoder_state, -1)\n",
    "      \n",
    "      for j in range(target_out.shape[1]):\n",
    "        logits, decoder_state = decoder([ids,\n",
    "                                         decoder_state,\n",
    "                                         encoder_outputs,\n",
    "                                         tf.sign(source)],\n",
    "                                         training=is_training)\n",
    "        ids = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "        generated = tf.concat((generated, tf.expand_dims(ids, 1)), axis=1)\n",
    "\n",
    "      seq_lens = tf.argmax(tf.cast(tf.equal(target_out, 2), tf.int32), axis=1)\n",
    "      for pred, tgt, seq_len in zip(generated.numpy(), target_out.numpy(), seq_lens.numpy()):\n",
    "        pred = pred[1:][:seq_len+1]\n",
    "        tgt = tgt[:seq_len+1]\n",
    "        matched = np.all(pred == tgt)\n",
    "        m.update_state(int(matched))\n",
    "    \n",
    "    acc = m.result().numpy()\n",
    "    logger.info(\"Evaluation: Testing Exact Match Accuracy: {:.3f}\".format(acc))\n",
    "    history_acc.append(acc)\n",
    "\n",
    "    if acc > best_acc:\n",
    "      best_acc = acc\n",
    "    logger.info(\"Best Accuracy: {:.3f}\".format(best_acc))\n",
    "\n",
    "    if len(history_acc) > params['num_patience'] and is_descending(history_acc):\n",
    "      logger.info(\"Testing Accuracy not improved over {} epochs, Early Stop\".format(params['num_patience']))\n",
    "      break\n",
    "  else:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Seq2seq with GRU\n",
    "\n",
    "앞 실습에서는 seq2seq의 Base RNN unit을 LSTM으로 사용했습니다. 이를 GRU로 대체해서 Encoder, Decoder를 새롭게 설계해서 제출해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    \n",
    "    #### Assignment 2-1 ####\n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    \n",
    "    #### Assignment 2-2 ####\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
